<!DOCTYPE html
    PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">

<head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Microsoft Word - Wearable-sensory-modifiers.docx</title>
    <style type="text/css">
        * {
            margin: 0;
            padding: 0;
            text-indent: 0;
        }
       img{
        margin: auto;
       }
        body {
            max-width: 90%;
            margin: auto;
        }

        .s1 {
            color: black;
            font-family: "Linux Biolinum O", sans-serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 17pt;
        }

        .s2 {
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 12pt;
        }

        .a,
        a {
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 10pt;
        }

        .p,
        p {
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 10pt;
            margin: 0pt;
        }

        .s4 {
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 10pt;
        }

        h1 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 11pt;
        }

        .s5 {
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        h2 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 10pt;
        }

        .s6 {
            color: black;
            font-family: "Linux Biolinum O", sans-serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        .s7 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: italic;
            font-weight: normal;
            text-decoration: none;
            font-size: 10pt;
        }

        .h3 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 9pt;
        }

        .s8 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: italic;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        .s9 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 10pt;
        }

        .s10 {
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        .s11 {
            color: #00F;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: underline;
            font-size: 9pt;
        }

        .s12 {
            color: #00F;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: underline;
            font-size: 9pt;
        }

        li {
            display: block;
        }

        #l1 {
            padding-left: 0pt;
        }

        #l1>li>*:first-child:before {
            content: "• ";
            color: black;
            font-family: "Linux Libertine Display O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        li {
            display: block;
        }

        #l2 {
            padding-left: 0pt;
            counter-reset: d1 1;
        }

        #l2>li>*:first-child:before {
            counter-increment: d1;
            content: counter(d1, decimal)" ";
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 10pt;
        }

        #l2>li:first-child>*:first-child:before {
            counter-increment: d1 0;
        }

        #l3 {
            padding-left: 0pt;
            counter-reset: d2 1;
        }

        #l3>li>*:first-child:before {
            counter-increment: d2;
            content: counter(d1, decimal)"." counter(d2, decimal)" ";
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 10pt;
        }

        #l3>li:first-child>*:first-child:before {
            counter-increment: d2 0;
        }

        #l4 {
            padding-left: 0pt;
            counter-reset: d2 1;
        }

        #l4>li>*:first-child:before {
            counter-increment: d2;
            content: counter(d1, decimal)"." counter(d2, decimal)" ";
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 10pt;
        }

        #l4>li:first-child>*:first-child:before {
            counter-increment: d2 0;
        }

        #l5 {
            padding-left: 0pt;
            counter-reset: d2 1;
        }

        #l5>li>*:first-child:before {
            counter-increment: d2;
            content: counter(d1, decimal)"." counter(d2, decimal)" ";
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: bold;
            text-decoration: none;
            font-size: 10pt;
        }

        #l5>li:first-child>*:first-child:before {
            counter-increment: d2 0;
        }

        li {
            display: block;
        }

        #l6 {
            padding-left: 0pt;
            counter-reset: e1 1;
        }

        #l6>li>*:first-child:before {
            counter-increment: e1;
            content: "[" counter(e1, decimal)"] ";
            color: black;
            font-family: "Linux Libertine O", serif;
            font-style: normal;
            font-weight: normal;
            text-decoration: none;
            font-size: 9pt;
        }

        #l6>li:first-child>*:first-child:before {
            counter-increment: e1 0;
        }

        img {
            margin: auto;
        }
    </style>
</head>

<body>
    <p class="s1" style="padding-top: 24pt;padding-left: 0;text-indent: 0pt;text-align: center;">Wearable sensory
        modifiers for better recognition learning</p>
    <p class="s2" style="padding-top: 11pt;padding-left: 0;text-indent: -2pt;text-align: center;">Christopher Romano<br>
        <a href="mailto:chris.romano@northwestern.edu" class="a" target="_blank">Feinberg School of Medicine<br>
            Northwestern University Chicago IL USA </a><br><a href="mailto:chris.romano@northwestern.edu"
            target="_blank">chris.romano@northwestern.edu</a></p>
    <p class="s2" style="padding-top: 11pt;padding-left: 0;text-indent: 0pt;line-height: 14pt;text-align: center;">
        Nabil Alshurafa</p>
    <p style="padding-left: 0;text-indent: 0pt;text-align: center;"><a href="mailto:nabil@northwestern.edu"
            class="s4" target="_blank">Feinberg School of Medicine<br> Northwestern University Chicago IL USA<br>
            nabil@northwestern.edu</a></p>
    <p style="text-indent: 0pt;text-align: left;"><br /></p>
    <p style="text-indent: 0pt;text-align: left;"><br /></p>
    <h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;">ABSTRACT</h1>
    <p class="s5" style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 110%;text-align: justify;">
        Many skills depend on recognition of specific features within raw streams of sensory input. In humans, feature
        detection (1) requires ordered cortical processing of sensory input and (2) improves with repetition. For most
        individuals, practice is the sole barrier to acquiring a stimulus-discrimination skill. However, individuals
        with disordered sensory processing often struggle to deeply cognize features when first encountering them,
        thereby impairing the development of robust mental schemata and limiting the benefit of additional practice.
        Such processing deficiencies, including those that do not meet clinical thresholds, constitute a disparity in
        the learning outcomes of students and/or trainees whose educational program involves developing feature
        recognition skills. Likening the predicament of such students to problems of machine learning provides a
        feasible approach to closing this disparity and a potential improvement to educational technologies in general.
        This paper presents the design of a wearable system that addresses a specific learning example (nursing and
        medical students learning to auscultate heart, lung, and bowel sounds) and proposes a general framework for the
        design of such systems across various recognition-dependent skills and their associated sensory channel.</p>
    <h1 style="padding-top: 10pt;padding-left: 7pt;text-indent: 0pt;text-align: left;">CCS CONCEPTS</h1>
    <ul id="l1">
        <li data-list-text="•">
            <p class="s5" style="padding-top: 2pt;padding-left: 12pt;text-indent: -5pt;text-align: left;">Human-centered
                computing → Human-computer interaction</p>
        </li>
        <li data-list-text="•">
            <p class="s5"
                style="padding-top: 1pt;padding-left: 7pt;text-indent: 0pt;line-height: 110%;text-align: left;">Applied
                computing → Education → Computer-assisted instruction • Computer systems organization → Real-time
                systems</p>
        </li>
    </ul>
    <p class="s5" style="padding-left: 7pt;text-indent: 0pt;line-height: 10pt;text-align: left;">→ Real-time system
        architecture</p>
    <p style="text-indent: 0pt;text-align: left;"><br /></p>
    <h1 style="padding-left: 7pt;text-indent: 0pt;text-align: left;">KEYWORDS</h1>
    <p class="s5" style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: justify;">
        Sensory processing, Wearable computing, Sensory modiﬁcation, Stimulus discrimination, Procedural learning,
        Auscultation</p>
    <p style="text-indent: 0pt;text-align: left;"><br /></p>
    <ol id="l2">
        <li data-list-text="1">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: left;">INTRODUCTION</h2>
            <p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">Humans
                can recognize a previously encountered stimulus with remarkable accuracy. This ability is made possible
                by repeated perception, encoding, and processing of the stimulus. For instance, guitarist Jack White is
                known for his ability to correctly identify any song by The Beatles after hearing only the first
                second[1]. White can do this because his brain is excellent at processing audio (encoding and
                processing) and because he has heard each Beatles song many times (repeated perception). Many human
                skills,</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <p style="padding-left: 7pt;text-indent: 0pt;line-height: 119%;text-align: justify;">occupational or
                otherwise, involve developing similar stimulus-discrimination abilities. This poses a challenge to
                individuals with substandard processing abilities, especially those exceeding clinical thresholds of
                disordered sensory processing (see section 1.1). Beyond the learning-outcome disparities to which these
                individuals are vulnerable, there exist problems of processing and stimulus-discrimination that are
                general to occupations, including the low accuracies of medical students (~20%) and attending physicians
                (~40%) in recognizing heart murmurs—crucial indicators of cardiac health—heard through a stethoscope[2].
            </p>
            <p style="padding-left: 19pt;text-indent: 0pt;line-height: 11pt;text-align: justify;">Wearable technologies
                can modify sensory input (e.g.,</p>
            <p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                amplifying sounds), and thus offer potential solutions to problems of sensory processing and
                stimulus-discrimination learning. Lessons learned from clinical studies of impaired sensory processing
                can be generalized to these technologies. The following subsections (1.1-1.4) address problems of
                disordered and/or deficient sensory processing and explicate the learning process behind
                stimulus-discrimination skills. Later sections detail real-world examples of occupational
                stimulus-discrimination skills (2), outline solutions to comparable problems of stimulus-discrimination
                found in machine learning (3) and present the design of a wearable system that addresses a real-world
                occupational skill- learning problem by applying insights from machine learning to an educational
                technology (4). Finally, sections 5 and 6 discuss how these insights can be generalized to the design of
                wearable systems that address a broader range of stimulus- discrimination learning problems.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <ol id="l3">
                <li data-list-text="1.1">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: left;">Disordered Sensory Processing
                    </h2>
                    <p
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Problems of sensory processing are distinct from problems of sensation. Problems of sensation
                        include those resulting from dysfunction of a sensory organ, such as blindness, which results
                        from dysfunctions of the eye and/or optic nerve[3]. Problems of sensory processing, however,
                        include those resulting from neural and/or cognitive abnormalities that arise in processing;
                        that is, only after the information has</p>
                    <p
                        style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        been successfully acquired through sensation. Abnormal sensory processing is conceptualized by
                        the umbrella term <span class="s7">Sensory Processing Disorder </span>(SPD)[4]<span class="s7">,
                        </span>referring to a multitude of processing problems including those of <span
                            class="s7">modulation[5] </span>(e.g., hypersensitivity) and <span class="s7">motor
                            control[6] </span>(e.g., speech production); as well as those of <span
                            class="s7">integration </span>and <span class="s7">discrimination[5, 7] </span>(e.g., the
                        inability of children with autism spectrum disorder [ASD] or attention deficit disorder [ADD] to
                        mentally assemble the noises they hear into meaningful sounds, such as by inferring the sound of
                        a door being opened from the noises made by the turning of a handle and the creaking of hinges).
                        Whether SPD describes an independent disorder, as opposed to an amalgam of symptoms comorbid to
                        established psychological disorders (namely ADD and ASD) remains under debate[8-10]. While this
                        uncertainty precludes the development of clinical therapies and diagnostic standards, the
                        contemporary understanding of sensory processing permits narrower assistive approaches.</p>
                    <p style="padding-top: 1pt;padding-left: 7pt;text-indent: 12pt;line-height: 118%;text-align: left;">
                        In other words, while we can only speculate about the causal mechanisms and treatment of SPD;
                        solutions to specific skill-learning problems that result from SPD-type symptomologies are
                        within reach. Prevalence of the symptomology associated with impaired sensory processing is
                        estimated at 5-16% of the US general population[9], indicating a sizable group that would
                        benefit from such solutions. Further, there necessarily exists a contingent of individuals with
                        less-pronounced sensory processing deficits that, by virtue of their minimal impact on everyday
                        life, do not merit clinical interest, yet remain quietly detrimental to discrimination-learning
                        outcomes relative to those of the average individual. Assuming some semblance of a spectrum to
                        exist between the lowest and highest processing capabilities, such solutions should, in
                        principle, benefit a population even larger than the population under discussion in the SPD
                        debate.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="1.2">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: left;">Learning to Discriminate Between
                        Stimuli</h2>
                    <p
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        The process of learning to recognize and discriminate between stimuli A and B (see Figure 1.)
                        can be expressed in two stages: (1) construction of independent cognitive schematics for both A
                        and B (&#39;independent&#39; insofar as they are somehow distinct from each other, as well as
                        from other stimuli in general) and (2) refinement of these schematics through additional
                        exposure to stimuli A and B. The present research focuses on the first stage (construction of
                        schematics), as this is likely where unequal sensory</p>
                    <p style="padding-top: 6pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: left;">
                        processing capabilities belie the greatest disparity of learning outcomes.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="1.3">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: left;">Feature Learning</h2>
                    <p style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: left;">
                        Feature learning is a machine learning concept with basis in cognitive neuropsychology[11].</p>
                    <p style="padding-left: 7pt;text-indent: 12pt;line-height: 118%;text-align: left;">
                        Electroencephalogram (EEG) studies of stimulus- discrimination learning tasks demonstrate the
                        brain&#39;s reliance on feature-learning to form robust concept representations. Event-related
                        potential (ERP) amplitudes to &quot;relevant attributes&quot;; i.e., features <span
                            class="s7">useful </span>to classification of a given stimulus as A, B, or neither; remain
                        substantially larger than ERP amplitudes to &quot;irrelevant attributes&quot;; i.e., features
                        <span class="s7">not useful </span>to classification of the stimulus; across repeated learning
                        tasks[12].</p>
                    <p style="padding-left: 7pt;text-indent: 12pt;line-height: 118%;text-align: left;">In other words,
                        as feature relevance is learned, attention is increasingly allocated towards features that
                        permit the most confident classifications of the stimuli. In addition to being ascribed
                        relevance (i.e., &quot;does this stimulus indicate something?&quot;), these features are
                        associated with the specific stimulus of which they are indicative (i.e., &quot;what does this
                        stimulus indicate?&quot;). The learning procedure is illustrated in Figure 1, and the execution
                        of the learned stimulus- discrimination task is illustrated in Figure 2.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                    <p style="padding-left: 7pt;text-indent: 0pt;text-align: left;"><span><img width="319" height="285"
                                alt="image" src="Wearable-sensory-modifiers_files/Image_001.png" /></span></p>
                    <h2
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Figure 1. <span class="p">Stimulus-discrimination learning. Features are represented as objects
                            that vary in shape and color. With repetition, the learner&#39;s mental model begins to
                            &#39;see through&#39; inter-trial variabilities and isolate key distinguishing features of
                            stimuli A and B.</span></h2>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                    <p style="text-indent: 0pt;text-align: left;"><span><img width="318" height="104" alt="image"
                                src="Wearable-sensory-modifiers_files/Image_002.png" /></span></p>
                    <p style="text-indent: 0pt;text-align: left;"><span><img width="169" height="186" alt="image"
                                src="Wearable-sensory-modifiers_files/Image_003.png" /></span></p>
                    <p style="padding-left: 271pt;text-indent: 0pt;line-height: 111%;text-align: justify;">diastole (the
                        relaxing of the heart following its contraction during systole)[14]. Sonic characteristics of
                        this and other heart murmurs are visualized in figure 3.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                    <h2 style="padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">Figure 2.
                        <span class="p">After learning (see Fig. 1), the subject can identify unlabeled stimuli by (1)
                            observing a key feature, (2) cross- referencing said feature against task-relevant mental
                            models, and (3) reporting the class to which the stimulus belongs.</span></h2>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="1.4">
                    <h2 style="padding-top: 6pt;padding-left: 22pt;text-indent: -15pt;text-align: left;">Stimulus
                        Interactions</h2>
                    <p
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 119%;text-align: justify;">
                        Figure 1. also illustrates factors that determine the intrinsic difficulty of constructing a
                        feature-based cognitive schematic of a stimulus. Stimulus similarity (determined by the number
                        and salience of key distinguishing features) and noise (features in the center of the Venn
                        Diagram in &#39;Learning Round X&#39; [see Fig. 1]) are both positively associated with learning
                        difficulty[13]. The differentiation of a stimulus, from both noise and other stimuli, is
                        critical to the formation of a stimulus-discrimination skill[11]; and is precisely the process
                        that many individuals with sensory-processing problems struggle with[10].</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
            </ol>
        </li>
        <li data-list-text="2">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: left;">AUSCULTATION</h2>
            <p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">For a
                specific example of an occupational stimulus- discrimination skill, consider auscultation—the use of a
                stethoscope to inspect the heart, lung, or bowels by listening to the sounds they produce. For instance,
                cardiac auscultation involves listening to heartbeats and identifying sonic features of abnormalities,
                such as heart murmurs, which are often indicative of cardiac pathologies[2]. Though often cast as a
                trivial element of medical education, auscultation is very difficult to learn, as evidenced by multiple
                studies reporting poor ability of medical students in recognizing heart murmurs (accuracy = approx.
                20%)[2]. Additional studies show that attending physicians correctly identify less than 40% of
                auscultated sounds[2]. It can be reasonably assumed that increasing these accuracies would improve care
                and care outcomes just as increasing the accuracy of any biomedical</p>
            <p style="padding-left: 7pt;text-indent: 0pt;text-align: justify;">monitoring instrument would.</p>
            <p style="padding-left: 7pt;text-indent: 12pt;line-height: 110%;text-align: left;">In terms of
                stimulus-discrimination, auscultation involves the detection of sonic features that indicate either
                normal organ function or one of multiple abnormalities that are known to occur in the organ. For
                instance, aortic valve regurgitation (a type of heart murmur) produces a sound that decreases in volume
                (decrescendo) and lasts throughout</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <h2 style="padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">Figure 3. <span
                    class="p">Phonocardiograms (visualized audio of heart sounds) including both normal sinus rhythm (A)
                    and several diastolic murmurs (B-F).</span></h2>
            <p style="padding-top: 11pt;padding-left: 7pt;text-indent: 12pt;line-height: 110%;text-align: left;">
                Auscultated sounds have three perceptible qualities: time (the temporal position of the sound in the
                cardiac cycle), pitch (the location of the sound on the audio-frequency spectrum), and intensity (the
                volume of the sound throughout its course). To recognize a particular sound, a subject must correctly
                perceive all three qualities and must be able to relate this perception to a pre-existing knowledge of
                heart sounds that includes both procedural (unconscious) knowledge of the sound in question and
                declarative (conscious) knowledge of the sound&#39;s associated murmur.</p>
            <p style="padding-left: 7pt;text-indent: 0pt;line-height: 110%;text-align: left;">Correctly perceiving the
                sounds requires differentiating them from the sounds of other organs, sounds created by movements and/or
                perturbations of the stethoscope&#39;s diaphragm, and ambient sounds of the surrounding environment.
                Thus, accurate auscultation requires both intact hearing and sufficient processing ability of the
                auditory cortex. Of course, there are additional factors relevant to the interpretation of an
                auscultated sound, such as the body-location the sound was collected from[14].</p>
            <p style="padding-left: 7pt;text-indent: 0pt;line-height: 109%;text-align: justify;">However, these factors
                constitute declarative knowledge, and as such are beyond the purview of learned stimulus-
                discrimination, a type of procedural knowledge.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
        </li>
        <li data-list-text="3">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: left;">CURRICULUM LEARNING</h2>
            <p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                Machine learning (ML) generally involves training computational neural networks to perform stimulus-
                discrimination tasks by repeatedly showing them labeled examples of the target and non-target
                stimuli[11]. Recent work has shown performance improvements in the training process through a method
                termed &quot;Curriculum learning&quot;.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <p style="text-indent: 0pt;text-align: left;"><span><img width="320" height="304" alt="image"
                        src="Wearable-sensory-modifiers_files/Image_004.png" /></span></p>
            <p style="padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">Curriculum-learned
                models are trained on a set of examples that are ordered by difficulty, i.e., models are first shown
                target and non-target stimuli that are &#39;easier&#39; to distinguish between, typically by virtue of
                the stimuli being very dissimilar (recall that decreasing stimulus similarity decreases the difficulty
                of discrimination learning)[15]. Once discrimination between &#39;easy&#39; stimuli is mastered, the
                model is shown target and non-target stimuli that gradually increase in the difficulty of their
                classification, typically by</p>
            <p style="padding-left: 19pt;text-indent: -12pt;text-align: justify;">virtue of being increasingly similar
                or decreasingly salient.</p>
            <p style="padding-left: 7pt;text-indent: 12pt;line-height: 110%;text-align: left;">Untrained ML models
                effectively resemble a brain with disordered sensory discrimination processes. Both &#39;perceive&#39;
                inputs perfectly fine, but neither quite know <span class="s7">what </span>they are perceiving. In the
                case of ML, the curriculum learning approach acknowledges that the principal challenges of
                discrimination learning are front-loaded; that is, within the initial process of developing a
                first-draft cognitive</p>
            <p style="padding-left: 7pt;text-indent: 0pt;line-height: 112%;text-align: left;">schematic of a stimulus
                that may later be refined and/or elaborated to increase recognition accuracy.</p>
            <p style="padding-left: 7pt;text-indent: 12pt;line-height: 110%;text-align: left;">Wearable technologies
                enable adaptation of the curriculum learning strategy to human learning by subtly modifying sensory
                inputs to create &#39;easier&#39; discrimination tasks for initial learning trials. The following
                section details one such application.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
        </li>
        <li data-list-text="4">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: left;">THE ADJUSTABLE FREQUENCY STETHOSCOPE
            </h2>
            <p style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">To
                demonstrate how wearable sensor technology can address the problems of stimulus-discrimination learning
                discussed above, the Adjustable Frequency Stethoscope (AFS) is presented.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <ol id="l4">
                <li data-list-text="4.1">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: left;">Design</h2>
                    <p
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        The AFS is created by inserting a small electronic module between the tubing and headset of a
                        standard stethoscope. Incoming sounds are converted to digital audio by a transducer, filtered,
                        and outputted to the headset by a second transducer. The filtering process is governed by the
                        position of the filter selection knob, which allows users to calibrate the device to the
                        organ-appropriate setting (heart, lung, or bowel). A switch allows filtering to be enabled and
                        disabled at will.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                    <p style="padding-left: 5pt;text-indent: 0pt;line-height: 1pt;text-align: left;"><span><img
                                width="324" height="1" alt="image"
                                src="Wearable-sensory-modifiers_files/Image_005.png" /></span></p>
                    <p class="s5" style="padding-left: 7pt;text-indent: 0pt;line-height: 110%;text-align: left;"><span
                            class="h3">Figure 5. </span>(<span class="s8">right</span>) Diagram of the approximate
                        pockets of the audio frequency spectrum that each organ&#39;s sounds inhabit. If the AFS is set
                        to listen for heart sounds, only the range of the frequency spectrum contained within the red
                        semi-circle would be audible through the device; and the same is true for the other organ sounds
                        shown on the diagram.</p>
                    <h2
                        style="padding-top: 7pt;padding-left: 5pt;text-indent: 0pt;line-height: 119%;text-align: justify;">
                        Figure 4. <span class="p">Adjustable Frequency Stethoscope (AFS) shown exploded, parts labeled.
                            Auscultated sounds are selectively filtered (via the processing unit) before reaching the
                            headset.</span></h2>
                    <p
                        style="padding-top: 10pt;padding-left: 5pt;text-indent: 12pt;line-height: 111%;text-align: left;">
                        The AFS is designed to decrease the perceived similarity of heart, lung, and bowel sounds,
                        thereby easing their differentiation by the untrained ear (see section 1.4).</p>
                    <p style="padding-left: 5pt;text-indent: 12pt;line-height: 109%;text-align: left;">For heart sounds,
                        the AFS simply filters out all sounds outside of the range that heart sounds occupy (~20-
                        150hz)[16].</p>
                    <p style="padding-left: 5pt;text-indent: 12pt;line-height: 110%;text-align: left;">The same approach
                        is taken for bowel sounds, which span a larger range (150hz-5khz)[17] and are therefore noisier
                        post-filtering. However, bowel sounds are not clinically assessed by their character, but rather
                        by their presence and rate. Thus, the bar of adequate auditory processing is lowest for bowel
                        sounds.</p>
                    <p style="text-indent: 0pt;text-align: left;"><span><img width="278" height="134" alt="image"
                                src="Wearable-sensory-modifiers_files/Image_006.png" /></span></p>
                    <p style="padding-left: 5pt;text-indent: 12pt;line-height: 110%;text-align: left;">Finally, for lung
                        sounds, multiple settings are necessary, as breathing complications present in different
                        frequency ranges than do normal vesicular lung sounds (vesicular = 60-600hz, wheezing &gt;=
                        1khz, stridor &gt;= 2khz)[16]. The AFS can harness the frequency-dissimilarity of normal and
                        pathological lung sounds into an enhanced learning experience that explicitly and obviously
                        separates the two.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                    <p style="padding-top: 6pt;padding-left: 7pt;text-indent: 12pt;line-height: 110%;text-align: left;">
                        Removing the irrelevant frequencies allows learners who would otherwise struggle to pick the
                        target sound out of unfiltered audio to familiarize themselves with the sound in isolation,
                        facilitating the development of a mental representation of the sound <span class="s7">as they
                            experience it </span>(i.e., with respect to any perceptual and/or phenomenological variance
                        between learners) before attempting to recognize the sound within the noisy raw audio feed of a
                        standard analog stethoscope. Thus, teaching auscultation with the AFS subverts the barriers to
                        learning that are presented by auditory processing deficits. In fact, since the AFS reduces both
                        the time and effort necessary to learn auscultation, it may prove beneficial to the learning
                        outcomes of <span class="s7">all </span>students—not just those with deficient or disordered
                        auditory processing.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="4.2">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Form factor</h2>
                    <p
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 119%;text-align: justify;">
                        The AFS intentionally avoids altering the appearance of the standard stethoscope to preserve
                        semiotic value, as the stethoscope is consistently shown to be a powerful symbolic trust-inducer
                        in the healthcare environment[18]. Additionally, changes to the form factor could potentially
                        introduce learning transference issues when AFS learners use standard stethoscopes[19]. Finally,
                        minimal alteration was needed, as the stethoscope already constitutes a selective sensory input
                        channel, and ergo an ideal housing for the AFS, a sensory modification device.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
            </ol>
        </li>
        <li data-list-text="5">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: justify;">WEARABLE SENSORY MODIFERS</h2>
            <p class="s9"
                style="padding-top: 4pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                Wearable devices already exist—and in some cases are already popularized—in form factors that permit the
                modification of sensory input (e.g., glasses[20], earphones[21]). Afforded this precedent, we may begin
                to extend wearables into the space of assistive educational technologies that are beneficial to
                populations including and beyond those with symptom severities that exceed clinical thresholds.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
            <ol id="l5">
                <li data-list-text="5.1">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Auditory Modification</h2>
                    <p class="s9"
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        The AFS leverages the form factor of the stethoscope, though auditory modification may also be
                        achieved with headphones, hats, or hearing aids. More sophisticated techniques for processing
                        audio are also potentially useful, including pitch and formant shifting, multiband compression,
                        and Fast Fourier Transformation (FFT). Systems capable of providing sensory feedback to the user
                    </p>
                    <p class="s9"
                        style="padding-top: 8pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        could prove useful in musical learning scenarios, among others.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="5.2">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Visual Modification</h2>
                    <p class="s9"
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Sunglasses and eyeglasses are commonplace, and computerized glasses continue to become more
                        advanced, offering the possibility of visual modulation systems. Filtering the visible light
                        spectrum may prove useful, as could expanding it. A system combining eye-tracking and artificial
                        blurring could effectively superimpose the gaze- direction of one person onto the vision of
                        another—a technique already being explored in computer displays for online learning
                        environments[22].</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="5.3">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Olfactory Modification</h2>
                    <p class="s9"
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Olfactory modification is possible with masks, though the public use of masks will likely
                        decline as the prevalence of COVID-19 decreases. However, air quality and airborne germ concerns
                        have created a global market for nose and/or mouth-fitted air filtration devices[23]; indicating
                        that nasal-based wearable form factors may prove viable at scale. Smells could be mechanically
                        filtered, chemically modified, or combined with additional smells pursuant to the learning
                        objective.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="5.4">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Gustatory Modification</h2>
                    <p class="s9"
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Gustatory modification is possible in theory, though modifications are likely best applied
                        directly to the tasted object, rather than through a mechanical or chemical interaction with the
                        taste buds and/or saliva. Should a use- case for a wearable gustatory modifier become apparent,
                        the system would be best applied in the form of a false tooth or retainer.</p>
                    <p style="text-indent: 0pt;text-align: left;"><br /></p>
                </li>
                <li data-list-text="5.5">
                    <h2 style="padding-left: 22pt;text-indent: -15pt;text-align: justify;">Tactile Modification</h2>
                    <p class="s9"
                        style="padding-top: 2pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">
                        Finally, while the possibility of tactile modification is established, the theoretical
                        connections between extant form factors (piezoelectric clothing, mechanical and/or robotic
                        gloves) and learning tasks are more complex than with other senses. For example, it is
                        conceivable that Braille learners may benefit from a glove that changes temperature in response
                        to semantic features of felt sentences. However, temperature is a limited informational medium,
                        and modification of pressure would likely require a system of immense complexity to yield any
                        quantifiable educational benefit.</p>
                </li>
            </ol>
        </li>
        <li data-list-text="6">
            <h2 style="padding-top: 9pt;padding-left: 19pt;text-indent: -12pt;text-align: left;">DISCUSSION</h2>
            <p class="s9"
                style="padding-top: 5pt;padding-left: 7pt;text-indent: 0pt;line-height: 118%;text-align: justify;">Brain
                and computing sciences have long proved symbiotic. Just as neural and psychological principles inform
                the design of computational models, so too do results of experimental modeling inform the approach of
                brain research. The sophistication of contemporary computing and the pervasion of technologies into
                everyday life present an opportunity for this relationship to manifest <i>in situ, </i>such that
                wearable computing devices begin to &#39;fill in the gaps&#39; of human cognition, providing support
                where once was only limitation.</p>
            <p style="padding-left: 7pt;text-indent: 12pt;line-height: 118%;text-align: justify;">While form factor
                elegance is advisable, the target application of sensory modification devices—improved learning of
                sensory processing skills—allows designers to err on the side of technical performance maximization, as
                high- burden wearables are more permissible in educational scenarios than as lifestyle accessories.
                However, this is not to suggest that more is always better. When dealing with cognition, a light touch
                is often preferable.</p>
            <p style="text-indent: 0pt;text-align: left;"><br /></p>
        </li>
        <li data-list-text="7">
            <h2 style="padding-left: 19pt;text-indent: -12pt;text-align: left;">REFERENCES</h2>
        </li>
    </ol>
    <ol id="l6">
        <li data-list-text="[1]">
            <p class="s10" style="padding-top: 3pt;padding-left: 25pt;text-indent: -18pt;text-align: justify;">S.
                Kornhaber. (2022) The Vindication of Jack White. <i>The Atlantic</i>.</p>
        </li>
        <li data-list-text="[2]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">M. J. Barrett, C. S.
                Lacey, A. E. Sekara, E. A. Linden, and E. J. Gracely, &quot;Mastering cardiac murmurs: the power of
                repetition,&quot; <i>Chest, </i>vol. 126, no. 2, pp. 470-5, Aug 2004, doi: 10.1378/chest.126.2.470.</p>
        </li>
        <li data-list-text="[3]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">S. Y. Lee and F. B.
                Mesfin, &quot;Blindness. BTI - StatPearls,&quot; (in eng).</p>
        </li>
        <li data-list-text="[4]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">S. Mulligan,
                &quot;Patterns of Sensory Integration Dysfunction: A Confirmatory Factor Analysis,&quot; <i>The American
                    Journal of Occupational Therapy, </i>vol. 52, no. 10, pp. 819-828, 1998, doi:
                10.5014/ajot.52.10.819.</p>
        </li>
        <li data-list-text="[5]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">L. J. Miller, M. E.
                Anzalone, S. J. Lane, S. A. Cermak, and E. T. Osten, &quot;Concept evolution in sensory integration: a
                proposed nosology for diagnosis,&quot; <i>Am J Occup Ther, </i>vol. 61, no. 2, pp. 135-40, Mar-Apr 2007,
                doi: 10.5014/ajot.61.2.135.</p>
        </li>
        <li data-list-text="[6]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">W. N. Bair, T. Kiemel, J.
                J. Jeka, and J. E. Clark, &quot;Development of multisensory reweighting is impaired for quiet stance
                control in children with developmental coordination disorder (DCD),&quot; <i>PLoS One, </i>vol. 7, no.
                7, p. e40932, 2012, doi: 10.1371/journal.pone.0040932.</p>
        </li>
        <li data-list-text="[7]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">H. Lonkar, &quot;An
                Overview of Sensory Processing Disorder,&quot; <i>Honors Theses, </i>vol. 2444, 2014. [Online].
                Available: <span class="s11">https://scholarworks.wmich.edu/honors_theses/2444</span>.</p>
        </li>
        <li data-list-text="[8]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">T. Smith, D. W. Mruzek,
                and D. Mozingo, &quot;Sensory Integration Therapy,&quot; in <i>Controversial Therapies for Autism and
                    Intellectural Disabilities</i>, R. M. Foxx and J. A. Mulick Eds.: Routledge, 2015, ch. 15, p. 23.
            </p>
        </li>
        <li data-list-text="[9]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">L. J. Miller, S. A.
                Schoen, S. Mulligan, and J. Sullivan, &quot;Identification of Sensory Processing and Integration Symptom
                Clusters: A Preliminary Study,&quot; <i>Occup Ther Int, </i>vol. 2017, p. 2876080, 2017, doi:
                10.1155/2017/2876080.</p>
        </li>
        <li data-list-text="[10]">
            <p class="s10" style="padding-top: 7pt;padding-left: 25pt;text-indent: -18pt;text-align: justify;">D. E.
                Bamiou, F. E. Musiek, and L. M. Luxon, &quot;Aetiology and clinical presentations of auditory processing
                disorders--a review,&quot; <i>Arch Dis Child, </i>vol. 85, no. 5, pp. 361-5, Nov 2001, doi:
                10.1136/adc.85.5.361.</p>
        </li>
        <li data-list-text="[11]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">Y. Bengio, A. Courville,
                and P. Vincent, &quot;Representation learning: a review and new perspectives,&quot; <i>IEEE Trans
                    Pattern Anal Mach Intell, </i>vol. 35, no. 8, pp. 1798-828, Aug 2013, doi: 10.1109/TPAMI.2013.50.
            </p>
        </li>
        <li data-list-text="[12]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">F. Rösler,
                &quot;Event-related brain potentials in a stimulus- discrimination learning paradigm,&quot;
                <i>Psychophysiology, </i>vol. 18, no. 4, pp. 447-455, 1981, doi: 10.1111/j.1469- 8986.1981.tb02478.x.
            </p>
        </li>
        <li data-list-text="[13]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">T. R. Zentall and T. S.
                Clement, &quot;Simultaneous discrimination learning: Stimulus interactions,&quot; <i>Animal Learning
                    &amp; Behavior, </i>vol. 29, no. 4, pp. 311-325, 2001, doi: 10.3758/BF03192898.</p>
        </li>
        <li data-list-text="[14]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">L. S. Bickley, P. G.
                Szilagyi, and B. Bates, <i>Bates’ Guide to Physical Examination and History Taking</i>, 10 ed.
                Lippincott Williams &amp; Wilkins, 2009.</p>
        </li>
        <li data-list-text="[15]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">Y. Bengio, J. Louradour,
                R. Collobert, and J. Weston, &quot;Curriculum learning,&quot; presented at the Proceedings of the 26th
                Annual International Conference on Machine Learning, Montreal, Quebec, Canada, 2009. [Online].
                Available: <span class="s11">https://doi.org/10.1145/1553374.1553380</span>.</p>
        </li>
        <li data-list-text="[16]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">T. Falk, W.-Y. Chan, E.
                Sejdic, and T. Chau, &quot;Spectro- Temporal Analysis of Auscultatory Sounds,&quot; 2010.</p>
        </li>
        <li data-list-text="[17]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">D. Zaborski, M. Halczak,
                W. Grzesiak, and A. Modrzejewski, &quot;Recording and Analysis of Bowel Sounds,&quot; <i>Euroasian J
                    Hepatogastroenterol, </i>vol. 5, no. 2, pp. 67-73, Jul-Dec 2015, doi:
                10.5005/jp-journals-10018-1137.</p>
        </li>
        <li data-list-text="[18]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">M. Jiwa, S. Millett, X.
                Meng, and V. M. Hewitt, &quot;Impact of the presence of medical equipment in images on viewers&#39;
                perceptions of the trustworthiness of an individual on- screen,&quot; <i>J Med Internet Res, </i>vol.
                14, no. 4, p. e100, Jul 10 2012, doi: 10.2196/jmir.1986.</p>
        </li>
        <li data-list-text="[19]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">C. J. Schramke and R. M.
                Bauer, &quot;State-dependent learning in older and younger adults,&quot; <i>Psychol Aging, </i>vol. 12,
                no. 2, pp. 255-62, Jun 1997, doi: 10.1037//0882-7974.12.2.255.</p>
        </li>
        <li data-list-text="[20]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">V. Danry, P.
                Pataranutaporn, Y. Mao, and P. Maes, &quot;Wearable Reasoner: Towards Enhanced Human Rationality Through
                A Wearable Device With An Explainable AI Assistant,&quot; presented at the Proceedings of the Augmented
                Humans International Conference, Kaiserslautern, Germany, 2020. [Online]. Available: <span
                    class="s11">https://doi.org/10.1145/3384657.3384799</span>.</p>
        </li>
        <li data-list-text="[21]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">S. Zhang <i>et al.</i>,
                &quot;Coughtrigger: Earbuds IMU Based Cough Detection Activator Using An Energy-Efficient Sensitivity-
                Prioritized Time Series Classifier,&quot; in <i>ICASSP 2022 - 2022 IEEE International Conference on
                    Acoustics, Speech and Signal Processing (ICASSP)</i>, 23-27 May 2022 2022, pp. 1-5, doi:
                10.1109/ICASSP43922.2022.9746334.</p>
        </li>
        <li data-list-text="[22]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">S. D&#39;Angelo and D.
                Gergle, &quot;An Eye For Design: Gaze Visualizations for Remote Collaborative Work,&quot; presented at
                the Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, Montreal QC, Canada,
                2018. [Online]. Available: <span class="s11">https://doi.org/10.1145/3173574.3173923</span>.</p>
        </li>
        <li data-list-text="[23]">
            <p class="s10" style="padding-left: 25pt;text-indent: -18pt;text-align: justify;">&quot;First Defense Nasal
                Screens International.&quot; <a href="http://www.filteryourlife.com/" class="s12"
                    target="_blank">https://</a><span class="s11">www.filteryourlife.com/</span> (accessed.</p>
        </li>
    </ol>
</body>

</html>